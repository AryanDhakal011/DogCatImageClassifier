{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DR-eO17geWu"
   },
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMefrVPCg-60"
   },
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "cByqJBmfvCCZ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "bvaYchk5z1nH",
    "outputId": "d27c4ed6-f84f-47d4-d556-71694bdf3e7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.10.0'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxQxCBWyoGPE"
   },
   "source": [
    "## Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvE-heJNo3GG"
   },
   "source": [
    "### Preprocessing the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "8K2oNixn6HK0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "                                   rescale = 1./255, #applying feature scaling by dividing by 255\n",
    "                                   shear_range = 0.2, # Shear mapping\n",
    "                                   zoom_range = 0.2, #randomly zooming inside the pictures\n",
    "                                   horizontal_flip = True) # randomly flipping hte image horizontally\n",
    "#connecting train_datagen to our training set\n",
    "training_set = train_datagen.flow_from_directory(\"dataset/training_set\",\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrCMmGw9pHys"
   },
   "source": [
    "### Preprocessing the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "uvM2ATa2-E6V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af8O4l90gk7B"
   },
   "source": [
    "## Part 2 - Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ces1gXY2lmoX"
   },
   "source": [
    "### Initialising the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "q6w2uoBwAc2S"
   },
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5YJj_XMl5LF"
   },
   "source": [
    "### Step 1 - Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "ZPJhuDoAAiUe"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, activation = 'relu', input_shape =[64, 64, 3]))\n",
    "#filtes are the feature detectors , kernel size is the size of the Feature detector, we specify the activation function, and we resized the image to 64x64x3 (for rgb image) in preprocessing so we use it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tf87FpvxmNOJ"
   },
   "source": [
    "### Step 2 - Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "M0WSvijlC-T8"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import MaxPool2D\n",
    "cnn.add(MaxPool2D(pool_size=2, strides=2))\n",
    "# adding pooling layers to the convolution layers ,we are specifing the pool size(2x2), we are shifting 2x2 features in our feature map(Recommended), s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xaTOgD8rm4mU"
   },
   "source": [
    "### Adding a second convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "KEb7M32-EWla"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, activation = 'relu'))\n",
    "cnn.add(MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmiEuvTunKfk"
   },
   "source": [
    "### Step 3 - Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "OlvTokoaEeUv"
   },
   "outputs": [],
   "source": [
    "#flattening the layers into a 1 dimensional vector\n",
    "cnn.add(tf.keras.layers.Flatten()) #doesnt take anything as parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAoSECOm203v"
   },
   "source": [
    "### Step 4 - Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "8KInFDevE1AZ"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units = 128, activation = 'relu')) # units are the number of hidden neurals(larger number for images) and the activation function for the connection(reul recommended unless you have reached the final layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTldFvbX28Na"
   },
   "source": [
    "### Step 5 - Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "9ZXTyppvFQXM"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid')) #recommended to have sigmoid function for binary classificaiton, if we were doing multi class clasification, we would use softmax funciton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6XkI90snSDl"
   },
   "source": [
    "## Part 3 - Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfrFQACEnc6i"
   },
   "source": [
    "### Compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "biZTwpuGIUxS"
   },
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam', loss ='binary_crossentropy', metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehS-v3MIpX2h"
   },
   "source": [
    "### Training the CNN on the Training set and evaluating it on the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "A9lDoL8QIrVp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 37s 144ms/step - loss: 0.6778 - accuracy: 0.5724 - val_loss: 0.6416 - val_accuracy: 0.6505\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 0.6316 - accuracy: 0.6481 - val_loss: 0.5823 - val_accuracy: 0.6905\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 0.5710 - accuracy: 0.7079 - val_loss: 0.5277 - val_accuracy: 0.7310\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 0.5311 - accuracy: 0.7294 - val_loss: 0.5053 - val_accuracy: 0.7550\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 0.5101 - accuracy: 0.7458 - val_loss: 0.5097 - val_accuracy: 0.7665\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 0.4888 - accuracy: 0.7630 - val_loss: 0.5008 - val_accuracy: 0.7580\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.4618 - accuracy: 0.7756 - val_loss: 0.5246 - val_accuracy: 0.7450\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 43s 172ms/step - loss: 0.4571 - accuracy: 0.7772 - val_loss: 0.4877 - val_accuracy: 0.7680\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 44s 175ms/step - loss: 0.4381 - accuracy: 0.7935 - val_loss: 0.4709 - val_accuracy: 0.7860\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 0.4251 - accuracy: 0.7995 - val_loss: 0.4647 - val_accuracy: 0.7885\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 0.4161 - accuracy: 0.8058 - val_loss: 0.4536 - val_accuracy: 0.7900\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.3999 - accuracy: 0.8163 - val_loss: 0.4751 - val_accuracy: 0.7855\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 43s 172ms/step - loss: 0.3837 - accuracy: 0.8259 - val_loss: 0.5153 - val_accuracy: 0.7765\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 0.3664 - accuracy: 0.8372 - val_loss: 0.5016 - val_accuracy: 0.7900\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 42s 167ms/step - loss: 0.3578 - accuracy: 0.8418 - val_loss: 0.4796 - val_accuracy: 0.7855\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.3444 - accuracy: 0.8436 - val_loss: 0.4821 - val_accuracy: 0.7860\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 43s 172ms/step - loss: 0.3350 - accuracy: 0.8503 - val_loss: 0.5369 - val_accuracy: 0.7640\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 42s 167ms/step - loss: 0.3166 - accuracy: 0.8604 - val_loss: 0.4812 - val_accuracy: 0.7985\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 43s 172ms/step - loss: 0.3067 - accuracy: 0.8686 - val_loss: 0.5070 - val_accuracy: 0.7910\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 44s 175ms/step - loss: 0.2796 - accuracy: 0.8849 - val_loss: 0.5166 - val_accuracy: 0.7860\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.2802 - accuracy: 0.8794 - val_loss: 0.4880 - val_accuracy: 0.7985\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.2577 - accuracy: 0.8913 - val_loss: 0.5317 - val_accuracy: 0.7910\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 45s 180ms/step - loss: 0.2494 - accuracy: 0.8934 - val_loss: 0.5672 - val_accuracy: 0.7975\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 42s 168ms/step - loss: 0.2486 - accuracy: 0.8989 - val_loss: 0.5452 - val_accuracy: 0.7865\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 41s 165ms/step - loss: 0.2415 - accuracy: 0.9004 - val_loss: 0.5408 - val_accuracy: 0.8020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2646a895af0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x = training_set, validation_data = test_set, epochs = 25) # fit method trains the CNN on the training set, parameter for training set, validation data is the difference from what we did before. We repeat for 25 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3PZasO0006Z"
   },
   "source": [
    "## Part 4 - Making a single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "Uf5O2XbIJRI1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 56ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "test_image = image.load_img('dataset/single_prediction/dog_test.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1:\n",
    "  prediction = 'dog'\n",
    "else:\n",
    "  prediction = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "XidwrjwJuW9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
